{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Named Entity Recognition using Transformers\n\n- This notebook is inspired from the course by LazyProgrammer on Udemy: https://www.udemy.com/course/data-science-transformers-nlp/\n- There are few tweaking done as part of self-learning journey\n- Dataset: 'conll2003'\n","metadata":{}},{"cell_type":"markdown","source":"## Install and Import Packages","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers[torch] datasets seqeval","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:23:30.891515Z","iopub.execute_input":"2023-07-04T21:23:30.891874Z","iopub.status.idle":"2023-07-04T21:23:44.631709Z","shell.execute_reply.started":"2023-07-04T21:23:30.891845Z","shell.execute_reply":"2023-07-04T21:23:44.630281Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"\nimport numpy as np\n\nfrom datasets import load_dataset, load_metric\n\nfrom transformers import (AutoTokenizer, \n                          DataCollatorForTokenClassification, \n                          AutoModelForTokenClassification, \n                          TrainingArguments, \n                          Trainer)\n\nfrom huggingface_hub import notebook_login","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:32:22.462399Z","iopub.execute_input":"2023-07-04T21:32:22.462747Z","iopub.status.idle":"2023-07-04T21:32:23.179593Z","shell.execute_reply.started":"2023-07-04T21:32:22.462718Z","shell.execute_reply":"2023-07-04T21:32:23.178658Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-07-04T20:42:22.097558Z","iopub.execute_input":"2023-07-04T20:42:22.103081Z","iopub.status.idle":"2023-07-04T20:42:22.130595Z","shell.execute_reply.started":"2023-07-04T20:42:22.103006Z","shell.execute_reply":"2023-07-04T20:42:22.129562Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2228ec544b3f4d4b80a03925cb1750f0"}},"metadata":{}}]},{"cell_type":"markdown","source":"## The Data","metadata":{}},{"cell_type":"code","source":"# Load data from HuggingFace dataset\ndata = load_dataset('conll2003')\ndata","metadata":{"execution":{"iopub.status.busy":"2023-07-04T20:41:31.221583Z","iopub.execute_input":"2023-07-04T20:41:31.222452Z","iopub.status.idle":"2023-07-04T20:41:39.433671Z","shell.execute_reply.started":"2023-07-04T20:41:31.222408Z","shell.execute_reply":"2023-07-04T20:41:39.432727Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3867288b54e344cbaa45a0068520e886"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09fc10a52f6747b3b41e9ca816d28325"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset conll2003/conll2003 (download: 959.94 KiB, generated: 9.78 MiB, post-processed: Unknown size, total: 10.72 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f03cda28bd3c45f587c2a81f1d16474b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14042 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3454 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"910ef09f8b094e5dbf393f2c29ef5dd6"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 14042\n    })\n    validation: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3251\n    })\n    test: Dataset({\n        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n        num_rows: 3454\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Check a sample train data\ndata['train'][0]","metadata":{"execution":{"iopub.status.busy":"2023-07-04T20:43:18.566265Z","iopub.execute_input":"2023-07-04T20:43:18.567166Z","iopub.status.idle":"2023-07-04T20:43:18.580033Z","shell.execute_reply.started":"2023-07-04T20:43:18.567130Z","shell.execute_reply":"2023-07-04T20:43:18.578981Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'id': '0',\n 'tokens': ['EU',\n  'rejects',\n  'German',\n  'call',\n  'to',\n  'boycott',\n  'British',\n  'lamb',\n  '.'],\n 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"},"metadata":{}}]},{"cell_type":"code","source":"# Check features\ndata['train'].features","metadata":{"execution":{"iopub.status.busy":"2023-07-04T20:46:54.683473Z","iopub.execute_input":"2023-07-04T20:46:54.683848Z","iopub.status.idle":"2023-07-04T20:46:54.690220Z","shell.execute_reply.started":"2023-07-04T20:46:54.683818Z","shell.execute_reply":"2023-07-04T20:46:54.689336Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'id': Value(dtype='string', id=None),\n 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n 'pos_tags': Sequence(feature=ClassLabel(num_classes=47, names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n 'chunk_tags': Sequence(feature=ClassLabel(num_classes=23, names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n 'ner_tags': Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"},"metadata":{}}]},{"cell_type":"code","source":"# Check 'ner_tags' features\ndata['train'].features['ner_tags']","metadata":{"execution":{"iopub.status.busy":"2023-07-04T20:47:37.296983Z","iopub.execute_input":"2023-07-04T20:47:37.297520Z","iopub.status.idle":"2023-07-04T20:47:37.304276Z","shell.execute_reply.started":"2023-07-04T20:47:37.297489Z","shell.execute_reply":"2023-07-04T20:47:37.303417Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"},"metadata":{}}]},{"cell_type":"code","source":"# Check the names of features of 'ner_tags'\ndata['train'].features['ner_tags'].feature.names","metadata":{"execution":{"iopub.status.busy":"2023-07-04T20:48:19.424995Z","iopub.execute_input":"2023-07-04T20:48:19.425666Z","iopub.status.idle":"2023-07-04T20:48:19.431721Z","shell.execute_reply.started":"2023-07-04T20:48:19.425633Z","shell.execute_reply":"2023-07-04T20:48:19.430846Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"},"metadata":{}}]},{"cell_type":"code","source":"# Save feature names\nlabel_names = data['train'].features['ner_tags'].feature.names\nlabel_names","metadata":{"execution":{"iopub.status.busy":"2023-07-04T20:49:43.248263Z","iopub.execute_input":"2023-07-04T20:49:43.249030Z","iopub.status.idle":"2023-07-04T20:49:43.256668Z","shell.execute_reply.started":"2023-07-04T20:49:43.248997Z","shell.execute_reply":"2023-07-04T20:49:43.255687Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"# Select a transformer model\n# Cased model is selected as the 'Casing' matters in NER case\n# \"Bill\" in \"Bill Gates\" is a name of a person\n# \"bill\" in \"I paid the bill\" is an object.\nMODEL_CKPT = 'distilbert-base-cased'","metadata":{"execution":{"iopub.status.busy":"2023-07-04T20:50:20.849643Z","iopub.execute_input":"2023-07-04T20:50:20.850011Z","iopub.status.idle":"2023-07-04T20:50:20.854632Z","shell.execute_reply.started":"2023-07-04T20:50:20.849984Z","shell.execute_reply":"2023-07-04T20:50:20.853301Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Load the tokenizer for the model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T20:50:47.900490Z","iopub.execute_input":"2023-07-04T20:50:47.900856Z","iopub.status.idle":"2023-07-04T20:50:49.234167Z","shell.execute_reply.started":"2023-07-04T20:50:47.900825Z","shell.execute_reply":"2023-07-04T20:50:49.233206Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"588add5fb112474aa8b0a96ab75c47e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cd09a6db309495bb80a83e5fb6bd0ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef4e4dda898f4780b0b41f9a5e249613"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2265d989b5b745dc8bceac3b7fcff033"}},"metadata":{}}]},{"cell_type":"code","source":"# Sanity check on a random text\nidx = 0\nt = tokenizer(data['train'][idx]['tokens'], is_split_into_words=True)\nt.word_ids()","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:03:57.677112Z","iopub.execute_input":"2023-07-04T21:03:57.677494Z","iopub.status.idle":"2023-07-04T21:03:57.686363Z","shell.execute_reply.started":"2023-07-04T21:03:57.677461Z","shell.execute_reply":"2023-07-04T21:03:57.685221Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]"},"metadata":{}}]},{"cell_type":"code","source":"t.tokens()","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:04:20.032747Z","iopub.execute_input":"2023-07-04T21:04:20.033872Z","iopub.status.idle":"2023-07-04T21:04:20.041858Z","shell.execute_reply.started":"2023-07-04T21:04:20.033827Z","shell.execute_reply":"2023-07-04T21:04:20.039702Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"['[CLS]',\n 'EU',\n 'rejects',\n 'German',\n 'call',\n 'to',\n 'boycott',\n 'British',\n 'la',\n '##mb',\n '.',\n '[SEP]']"},"metadata":{}}]},{"cell_type":"markdown","source":"### Target Alignment\n\n- After Subword tokenization, the tokens won't allign with the targets. e.g. in above case: 'lamb' -> 'la', '##mb'\n- This issue needs to be fixed.\n- For any word split into multiple tokens, assign the same target.\n- Special tokens [CLS] and [SEP] tokens need to be accounted in the targets.","metadata":{}},{"cell_type":"code","source":"# Mapping \"begin_text\" to \"inside_text\"\nbegin2inside = {\n    1: 2, \n    3: 4, \n    5: 6, \n    7: 8\n}","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:07:08.968275Z","iopub.execute_input":"2023-07-04T21:07:08.968683Z","iopub.status.idle":"2023-07-04T21:07:08.973952Z","shell.execute_reply.started":"2023-07-04T21:07:08.968652Z","shell.execute_reply":"2023-07-04T21:07:08.972868Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def align_targets(labels, word_ids):\n    aligned_labels = []\n    last_word = None\n    for word in word_ids:\n        if word is None:\n            # Its a special token like [CLS]\n            label = -100 # HF transformers use -100 for special token\n        elif word != last_word:\n            # Its a new word\n            label = labels[word]\n        else:\n            # Its the same word as before\n            label = labels[word]\n            \n            # Change B-<tag> to I-<tag> if necessary\n            if label in begin2inside:\n                label = begin2inside[label]\n                \n        # Add the label\n        aligned_labels.append(label)\n        # Update the last word\n        last_word = word\n        \n    return aligned_labels","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:11:11.829720Z","iopub.execute_input":"2023-07-04T21:11:11.830138Z","iopub.status.idle":"2023-07-04T21:11:11.837505Z","shell.execute_reply.started":"2023-07-04T21:11:11.830104Z","shell.execute_reply":"2023-07-04T21:11:11.836499Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Try alignement function\nlabels = data['train'][idx]['ner_tags']\nword_ids = t.word_ids()\naligned_targets = align_targets(labels, word_ids)\naligned_targets","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:12:32.499176Z","iopub.execute_input":"2023-07-04T21:12:32.499869Z","iopub.status.idle":"2023-07-04T21:12:32.507778Z","shell.execute_reply.started":"2023-07-04T21:12:32.499836Z","shell.execute_reply":"2023-07-04T21:12:32.506783Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]"},"metadata":{}}]},{"cell_type":"code","source":"aligned_labels = [label_names[t] if t >= 0 else None for t in aligned_targets]\nfor x, y in zip(t.tokens(), aligned_labels):\n    print(f\"{x}\\t{y}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:14:02.555921Z","iopub.execute_input":"2023-07-04T21:14:02.556273Z","iopub.status.idle":"2023-07-04T21:14:02.562630Z","shell.execute_reply.started":"2023-07-04T21:14:02.556246Z","shell.execute_reply":"2023-07-04T21:14:02.561445Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[CLS]\tNone\nEU\tB-ORG\nrejects\tO\nGerman\tB-MISC\ncall\tO\nto\tO\nboycott\tO\nBritish\tB-MISC\nla\tO\n##mb\tO\n.\tO\n[SEP]\tNone\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to Tokenize both inputs and targets\ndef tokenize_fn(batch):\n    # Tokenize the input sequence first\n    # This populates input_ids, attention_mask, etc.\n    tokenized_inputs = tokenizer(\n    batch['tokens'], \n    truncation=True, \n    is_split_into_words=True)\n    # Original Targets\n    labels_batch = batch['ner_tags']\n    aligned_labels_batch = []\n    for i, labels in enumerate(labels_batch):\n        word_ids = tokenized_inputs.word_ids(i)\n        aligned_labels_batch.append(align_targets(labels, word_ids))\n        \n    tokenized_inputs['labels'] = aligned_labels_batch\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:18:43.901884Z","iopub.execute_input":"2023-07-04T21:18:43.902254Z","iopub.status.idle":"2023-07-04T21:18:43.909801Z","shell.execute_reply.started":"2023-07-04T21:18:43.902224Z","shell.execute_reply":"2023-07-04T21:18:43.908447Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Map the tokenize_fn to datasets\ntokenized_datasets = data.map(\n    tokenize_fn, \n    batched=True, \n    remove_columns=data['train'].column_names)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:20:04.988751Z","iopub.execute_input":"2023-07-04T21:20:04.989115Z","iopub.status.idle":"2023-07-04T21:20:08.582407Z","shell.execute_reply.started":"2023-07-04T21:20:04.989085Z","shell.execute_reply":"2023-07-04T21:20:08.581377Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11f89b05add6437ebd9239f568c7411c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"343f4848024c476293e58a8b2ae21ad9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"773a8625908849269544f4acf6973100"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:20:48.429798Z","iopub.execute_input":"2023-07-04T21:20:48.431153Z","iopub.status.idle":"2023-07-04T21:20:48.437701Z","shell.execute_reply.started":"2023-07-04T21:20:48.431117Z","shell.execute_reply":"2023-07-04T21:20:48.436678Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 14042\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 3251\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 3454\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Collator","metadata":{}},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:22:07.065088Z","iopub.execute_input":"2023-07-04T21:22:07.065654Z","iopub.status.idle":"2023-07-04T21:22:07.070706Z","shell.execute_reply.started":"2023-07-04T21:22:07.065611Z","shell.execute_reply":"2023-07-04T21:22:07.069505Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Example\nbatch = data_collator([tokenized_datasets['train'][i] for i in range(2)])\nbatch['labels']","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:22:46.797764Z","iopub.execute_input":"2023-07-04T21:22:46.798539Z","iopub.status.idle":"2023-07-04T21:22:46.841631Z","shell.execute_reply.started":"2023-07-04T21:22:46.798506Z","shell.execute_reply":"2023-07-04T21:22:46.840546Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],\n        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])"},"metadata":{}}]},{"cell_type":"markdown","source":"## Metric","metadata":{}},{"cell_type":"code","source":"# Instantiate the metric\nmetric = load_metric('seqeval')","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:26:38.784962Z","iopub.execute_input":"2023-07-04T21:26:38.785347Z","iopub.status.idle":"2023-07-04T21:26:39.372643Z","shell.execute_reply.started":"2023-07-04T21:26:38.785307Z","shell.execute_reply":"2023-07-04T21:26:39.371692Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05b5e573562b4fcd91e1bccea76edde7"}},"metadata":{}}]},{"cell_type":"code","source":"# Function to compute Metrics\ndef compute_metrics(logits_and_labels):\n    logits, labels = logits_and_labels\n    preds = np.argmax(logits, axis=-1)\n    \n    # Remove -100 from labels and predictions\n    # Convert the label_ids to label names\n    str_labels = [\n        [label_names[t] for t in label if t!= -100] for label in labels\n    ]\n    str_preds = [\n        [label_names[p] for p, t in zip(pred, targ) if t!= -100] for pred, targ in zip(preds, labels)\n    ]\n    metrics_ = metric.compute(predictions=str_preds, \n                            references=str_labels)\n    return {\n        'precision': metrics_['overall_precision'], \n        'recall': metrics_['overall_recall'], \n        'f1': metrics_['overall_f1'], \n        'accuracy': metrics_['overall_accuracy']\n    }","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:47:01.701186Z","iopub.execute_input":"2023-07-04T21:47:01.702248Z","iopub.status.idle":"2023-07-04T21:47:01.713652Z","shell.execute_reply.started":"2023-07-04T21:47:01.702209Z","shell.execute_reply":"2023-07-04T21:47:01.711173Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"## Fine-tune the model","metadata":{}},{"cell_type":"code","source":"# Create id2label, label2id dictionary\nid2label = {k: v for k, v in enumerate(label_names)}\nlabel2id = {v: k for k, v in id2label.items()}","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:33:43.772206Z","iopub.execute_input":"2023-07-04T21:33:43.773206Z","iopub.status.idle":"2023-07-04T21:33:43.780930Z","shell.execute_reply.started":"2023-07-04T21:33:43.773173Z","shell.execute_reply":"2023-07-04T21:33:43.779855Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Instantiate Model\nmodel = AutoModelForTokenClassification.from_pretrained(\n    MODEL_CKPT, \n    id2label=id2label, \n    label2id=label2id)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:35:03.391426Z","iopub.execute_input":"2023-07-04T21:35:03.391823Z","iopub.status.idle":"2023-07-04T21:35:06.373276Z","shell.execute_reply.started":"2023-07-04T21:35:03.391792Z","shell.execute_reply":"2023-07-04T21:35:06.372400Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/263M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d719851a191845838d6078c5ed8c18ab"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = f\"{MODEL_CKPT}-finetuned-CONLL2003\"","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:36:51.995369Z","iopub.execute_input":"2023-07-04T21:36:51.995846Z","iopub.status.idle":"2023-07-04T21:36:52.001089Z","shell.execute_reply.started":"2023-07-04T21:36:51.995811Z","shell.execute_reply":"2023-07-04T21:36:52.000185Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Training Arguments\ntraining_args = TrainingArguments(output_dir=model_name,\n                                  num_train_epochs=5,\n                                  learning_rate=2e-5,\n                                  weight_decay=0.01,\n                                  evaluation_strategy='epoch',\n                                  disable_tqdm=False,\n                                  push_to_hub=True,)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:38:15.020984Z","iopub.execute_input":"2023-07-04T21:38:15.021362Z","iopub.status.idle":"2023-07-04T21:38:15.067752Z","shell.execute_reply.started":"2023-07-04T21:38:15.021332Z","shell.execute_reply":"2023-07-04T21:38:15.066787Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Instantiate trainer\ntrainer = Trainer(model=model, \n                 args=training_args, \n                 train_dataset=tokenized_datasets['train'], \n                 eval_dataset=tokenized_datasets['validation'], \n                 data_collator=data_collator, \n                 compute_metrics=compute_metrics, \n                 tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:50:11.207082Z","iopub.execute_input":"2023-07-04T21:50:11.207481Z","iopub.status.idle":"2023-07-04T21:50:13.606323Z","shell.execute_reply.started":"2023-07-04T21:50:11.207441Z","shell.execute_reply":"2023-07-04T21:50:13.605118Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"/kaggle/working/distilbert-base-cased-finetuned-CONLL2003 is already a clone of https://huggingface.co/EulerianKnight/distilbert-base-cased-finetuned-CONLL2003. Make sure you pull the latest changes with `repo.git_pull()`.\n","output_type":"stream"}]},{"cell_type":"code","source":"# train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-07-04T21:50:15.895714Z","iopub.execute_input":"2023-07-04T21:50:15.896083Z","iopub.status.idle":"2023-07-04T22:00:56.891383Z","shell.execute_reply.started":"2023-07-04T21:50:15.896054Z","shell.execute_reply":"2023-07-04T22:00:56.890362Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8780' max='8780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8780/8780 10:40, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.030200</td>\n      <td>0.083189</td>\n      <td>0.905478</td>\n      <td>0.931841</td>\n      <td>0.918471</td>\n      <td>0.981191</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.024000</td>\n      <td>0.086726</td>\n      <td>0.923663</td>\n      <td>0.938741</td>\n      <td>0.931141</td>\n      <td>0.983296</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.012300</td>\n      <td>0.090936</td>\n      <td>0.922368</td>\n      <td>0.943790</td>\n      <td>0.932956</td>\n      <td>0.984473</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.005900</td>\n      <td>0.096248</td>\n      <td>0.921839</td>\n      <td>0.944800</td>\n      <td>0.933178</td>\n      <td>0.984370</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.002600</td>\n      <td>0.098283</td>\n      <td>0.927629</td>\n      <td>0.946988</td>\n      <td>0.937209</td>\n      <td>0.984812</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=8780, training_loss=0.014584794206065305, metrics={'train_runtime': 640.5511, 'train_samples_per_second': 109.609, 'train_steps_per_second': 13.707, 'total_flos': 767854087685244.0, 'train_loss': 0.014584794206065305, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub(commit_message='First Commit')","metadata":{"execution":{"iopub.status.busy":"2023-07-04T22:06:08.301481Z","iopub.execute_input":"2023-07-04T22:06:08.301887Z","iopub.status.idle":"2023-07-04T22:06:39.547020Z","shell.execute_reply.started":"2023-07-04T22:06:08.301856Z","shell.execute_reply":"2023-07-04T22:06:39.545841Z"},"trusted":true},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload file pytorch_model.bin:   0%|          | 1.00/249M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5452da2f5bb40e589db1b96c8d19b61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload file runs/Jul04_21-38-15_d673882b9f35/events.out.tfevents.1688507416.d673882b9f35.28.1:   0%|          …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53611b9222a44026a81c9b722dc58481"}},"metadata":{}},{"name":"stderr","text":"To https://huggingface.co/EulerianKnight/distilbert-base-cased-finetuned-CONLL2003\n   ec67e48..6c55bf3  main -> main\n\nTo https://huggingface.co/EulerianKnight/distilbert-base-cased-finetuned-CONLL2003\n   6c55bf3..e2420a8  main -> main\n\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"'https://huggingface.co/EulerianKnight/distilbert-base-cased-finetuned-CONLL2003/commit/6c55bf31d243bc403a209b78f453d045395c8132'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}